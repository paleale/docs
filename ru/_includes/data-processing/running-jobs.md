* [Создайте задания в {{ dataproc-name }}](../../data-proc/operations/jobs.md). После этого они запустятся автоматически.
* [Запустите задания Apache Hive](../../data-proc/tutorials/how-to-use-hive.md) с помощью {{ yandex-cloud }} CLI или Hive CLI.
* [Запустите приложения Spark или PySpark](../../data-proc/tutorials/run-spark-job.md) с помощью Spark Shell, утилиты `spark-submit` или {{ yandex-cloud }} CLI.
* [Запустите задания с удаленных хостов](../../data-proc/tutorials/remote-run-job.md), не входящих в кластер {{ dataproc-name }}, с помощью утилиты `spark-submit`.
* Настройте интеграцию с сервисом [{{ maf-full-name }}](../../data-proc/tutorials/airflow-automation.md) или [{{ ml-platform-full-name }}](../../data-proc/tutorials/datasphere-integration.md). Тогда запуск заданий будет автоматизирован.
